# =============================================================================
# Logstash Pipeline Configuration
# =============================================================================
# Consumes logs from Kafka topic (sent via Logback) and sends to Elasticsearch
# =============================================================================

input {
  # Application Logs from Logback Kafka Appender
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["order-system.logs"]
    group_id => "logstash-logs-consumer"
    codec => json
    decorate_events => true
    consumer_threads => 3
    auto_offset_reset => "earliest"
  }
}

filter {
  # Parse the @timestamp field
  date {
    match => ["@timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  # Add log category based on logger name
  if [logger_name] =~ /service\.CartService/ {
    mutate { add_field => { "log_category" => "cart" } }
  } else if [logger_name] =~ /service\.OrderService/ {
    mutate { add_field => { "log_category" => "order" } }
  } else if [logger_name] =~ /controller/ {
    mutate { add_field => { "log_category" => "api" } }
  } else if [logger_name] =~ /scheduler/ {
    mutate { add_field => { "log_category" => "scheduler" } }
  } else if [logger_name] =~ /exception/ {
    mutate { add_field => { "log_category" => "error" } }
  } else {
    mutate { add_field => { "log_category" => "general" } }
  }
  
  # Extract key fields from message if present
  grok {
    match => { 
      "message" => [
        "cartId=%{DATA:extracted_cartId}[,\s]",
        "orderId=%{DATA:extracted_orderId}[,\s]",
        "userId=%{DATA:extracted_userId}[,\s]",
        "total=%{NUMBER:extracted_total}"
      ]
    }
    tag_on_failure => []
  }
  
  # Remove redundant fields
  mutate {
    remove_field => ["@version", "host"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "order-system-logs-%{+YYYY.MM.dd}"
  }
  
  # Debug output (uncomment for troubleshooting)
  # stdout { codec => rubydebug }
}
